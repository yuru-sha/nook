# nook/functions/viewer/viewer.py
import os
import re
from datetime import datetime
from enum import Enum

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, JSONResponse, RedirectResponse
from fastapi.templating import Jinja2Templates

from nook.functions.common.python.gemini_client import create_client

app = FastAPI()
# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’çµ¶å¯¾ãƒ‘ã‚¹ã§æŒ‡å®š
templates = Jinja2Templates(
    directory=os.path.join(os.path.dirname(__file__), "templates")
)

# NOOK_TYPEã«åŸºã¥ã„ã¦ã‚¢ãƒ—ãƒªåã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨­å®š
NOOK_TYPE = os.environ.get("NOOK_TYPE", "default")


class NookType(str, Enum):
    """Nookã®ã‚¿ã‚¤ãƒ—ã‚’å®šç¾©ã™ã‚‹Enum"""

    DEFAULT = "default"
    CAMERA = "camera"


def get_app_names(nook_type: NookType) -> list[str]:
    """
    æŒ‡å®šã•ã‚ŒãŸNookã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚¢ãƒ—ãƒªåã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚

    Parameters
    ----------
    nook_type : NookType
        Nookã®ã‚¿ã‚¤ãƒ—

    Returns
    -------
    list[str]
        ã‚¢ãƒ—ãƒªåã®ãƒªã‚¹ãƒˆ
    """
    if nook_type == NookType.CAMERA:
        return [
            "reddit_explorer",
            "tech_feed",
        ]
    else:
        return [
            "github_trending",
            "hacker_news",
            "paper_summarizer",
            "reddit_explorer",
            "tech_feed",
        ]


def get_app_title(nook_type: NookType) -> str:
    """
    æŒ‡å®šã•ã‚ŒãŸNookã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚¢ãƒ—ãƒªã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿”ã™ã€‚

    Parameters
    ----------
    nook_type : NookType
        Nookã®ã‚¿ã‚¤ãƒ—

    Returns
    -------
    str
        ã‚¢ãƒ—ãƒªã‚¿ã‚¤ãƒˆãƒ«
    """
    return "Nook Camera" if nook_type == NookType.CAMERA else "Nook"


WEATHER_ICONS = {
    "100": "â˜€ï¸",
    "101": "ğŸŒ¤ï¸",
    "200": "â˜ï¸",
    "201": "â›…",
    "202": "ğŸŒ§ï¸",
    "300": "ğŸŒ§ï¸",
    "301": "ğŸŒ¦ï¸",
    "400": "ğŸŒ¨ï¸",
}


def get_weather_data():
    try:
        response = requests.get(
            "https://www.jma.go.jp/bosai/forecast/data/forecast/130000.json", timeout=5
        )
        response.raise_for_status()
        data = response.json()
        tokyo = next(
            (
                area
                for area in data[0]["timeSeries"][2]["areas"]
                if area["area"]["name"] == "æ±äº¬"
            ),
            None,
        )
        tokyo_weather = next(
            (
                area
                for area in data[0]["timeSeries"][0]["areas"]
                if area["area"]["code"] == "130010"
            ),
            None,
        )
        if tokyo and tokyo_weather:
            temps = tokyo["temps"]
            weather_code = tokyo_weather["weatherCodes"][0]
            weather_icon = WEATHER_ICONS.get(weather_code, "")
            return {
                "temp": temps[0],
                "weather_code": weather_code,
                "weather_icon": weather_icon,
            }
    except Exception as e:
        print(f"Error fetching weather data: {e}")
    return {
        "temp": "--",
        "weather_code": "100",
        "weather_icon": WEATHER_ICONS.get("100", "â˜€ï¸"),
    }


def extract_links(text: str) -> list[str]:
    markdown_links = re.findall(r"\[([^\]]+)\]\(([^)]+)\)", text)
    markdown_links = [
        (text, url)
        for text, url in markdown_links
        if not text.startswith("[Image]") and not text.startswith("[Video]")
    ]
    urls = re.findall(r"(?<![\(\[])(https?://[^\s\)]+)", text)
    return [url for _, url in markdown_links] + urls


def fetch_url_content(url: str) -> str | None:
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        for element in soup(["script", "style", "nav", "header", "footer"]):
            element.decompose()
        main_content = soup.find("article") or soup.find("main") or soup.find("body")
        if main_content:
            text = " ".join(main_content.get_text(separator=" ").split())
            return text[:1000] + "..." if len(text) > 1000 else text
        return None
    except Exception as e:
        print(f"Error fetching URL {url}: {e}")
        return None


def fetch_markdown(
    app_name: str, date_str: str, nook_type: NookType = NookType.DEFAULT
) -> str:
    """
    æŒ‡å®šã•ã‚ŒãŸã‚¢ãƒ—ãƒªåã¨æ—¥ä»˜ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã™ã‚‹ã€‚

    Parameters
    ----------
    app_name : str
        ã‚¢ãƒ—ãƒªå
    date_str : str
        æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
    nook_type : NookType
        Nookã®ã‚¿ã‚¤ãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯DEFAULTï¼‰

    Returns
    -------
    str
        ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®å†…å®¹
    """
    output_dir = os.environ.get("OUTPUT_DIR", "./output")
    key = f"{nook_type.value}/{app_name}/{date_str}.md"
    file_path = os.path.join(output_dir, key)
    try:
        with open(file_path, encoding="utf-8") as f:
            content = f.read()
            print(f"Fetched markdown for {key}:")
            print(content[:500])
            return content
    except Exception as e:
        return f"Error fetching {key}: {e}"


@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¿ã‚¤ãƒ—ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    return RedirectResponse(url="/default")


@app.get("/{nook_type}", response_class=HTMLResponse)
async def index(request: Request, nook_type: str):
    """
    æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ—ã®Nookã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤ºã€‚

    Parameters
    ----------
    request : Request
        ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    nook_type : str
        Nookã®ã‚¿ã‚¤ãƒ—ï¼ˆ"default" ã¾ãŸã¯ "camera"ï¼‰

    Returns
    -------
    HTMLResponse
        ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã®HTML
    """
    try:
        nook_type_enum = NookType(nook_type.lower())
    except ValueError:
        return RedirectResponse(url="/default")

    app_names = get_app_names(nook_type_enum)
    app_title = get_app_title(nook_type_enum)
    weather_data = get_weather_data()

    # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰æ—¥ä»˜ã‚’å–å¾—ã€ãªã‘ã‚Œã°ä»Šæ—¥ã®æ—¥ä»˜ã‚’ä½¿ç”¨
    date_str = request.query_params.get("date", datetime.now().strftime("%Y-%m-%d"))

    # å„ã‚¢ãƒ—ãƒªã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
    contents = {}
    for app_name in app_names:
        contents[app_name] = fetch_markdown(app_name, date_str, nook_type_enum)

    # ã‚¿ãƒ–åã‚’è¨­å®š
    tab_names = {
        "github_trending": "GitHub Trending",
        "hacker_news": "Hacker News",
        "paper_summarizer": "Paper Summarizer",
        "reddit_explorer": "Reddit Explorer",
        "tech_feed": "Tech Feed",
    }

    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "app_names": app_names,
            "app_title": app_title,
            "nook_type": nook_type_enum.value,
            "weather_data": weather_data,
            "contents": contents,
            "tab_names": tab_names,
            "date": date_str,  # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ—¥ä»˜ã‚’æ¸¡ã™
        },
    )


@app.get("/api/weather", response_class=JSONResponse)
async def get_weather():
    return get_weather_data()


_MESSAGE = """
ä»¥ä¸‹ã®è¨˜äº‹ã«é–¢é€£ã—ã¦ã€æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ã‚’ç”¨ã„ã¦äº‹å®Ÿã‚’ç¢ºèªã—ãªãŒã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦ã§ãã‚‹ã ã‘è©³ç´°ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ãªãŠã€å›ç­”ã¯Markdownå½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

[è¨˜äº‹]

{markdown}

{additional_context}

[ãƒãƒ£ãƒƒãƒˆå±¥æ­´]

'''
{chat_history}
'''

[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æ–°ã—ã„è³ªå•]

'''
{message}
'''

ãã‚Œã§ã¯ã€å›ç­”ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
"""


@app.post("/chat/{topic_id}")
async def chat(topic_id: str, request: Request):
    """
    ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚

    Parameters
    ----------
    topic_id : str
        ãƒˆãƒ”ãƒƒã‚¯ID
    request : Request
        ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    Returns
    -------
    dict
        ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    """
    data = await request.json()
    message = data.get("message")
    markdown = data.get("markdown")
    chat_history = data.get("chat_history", "ãªã—")
    links = extract_links(markdown) + extract_links(message)
    additional_context_list = []
    for url in links:
        if content := fetch_url_content(url):
            additional_context_list.append(
                f"- Content from {url}:\n\n'''{content}'''\n\n"
            )
    additional_context = (
        (
            "\n\n[è¨˜äº‹ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å«ã¾ã‚Œã‚‹ãƒªãƒ³ã‚¯ã®å†…å®¹](ã†ã¾ãå–å¾—ã§ãã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)\n\n"
            + "\n\n".join(additional_context_list)
        )
        if additional_context_list
        else ""
    )
    formatted_message = _MESSAGE.format(
        markdown=markdown,
        additional_context=additional_context,
        chat_history=chat_history,
        message=message,
    )
    gemini_client = create_client(use_search=True)
    response_text = gemini_client.chat_with_search(formatted_message)
    return {"response": response_text}


if __name__ == "__main__":
    load_dotenv(os.path.join(os.path.dirname(__file__), "..", "..", "..", ".env"))
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8080)
