import datetime
import os
import re

import boto3
import requests
from bs4 import BeautifulSoup
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from gemini_client import create_client
from mangum import Mangum

app = FastAPI()
templates = Jinja2Templates(directory="templates")

# S3ãƒã‚±ãƒƒãƒˆåã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
BUCKET_NAME = os.environ.get("BUCKET_NAME")
s3_client = boto3.client("s3")
gemini_model = "gemini-2.0-flash"

# å¯¾è±¡ã®ã‚¢ãƒ—ãƒªåãƒªã‚¹ãƒˆ
app_names = [
    "github_trending",
    "hacker_news",
    "paper_summarizer",
    "reddit_explorer",
    "tech_feed",
]

# å¤©æ°—ã‚¢ã‚¤ã‚³ãƒ³ã®å¯¾å¿œè¡¨
WEATHER_ICONS = {
    "100": "â˜€ï¸",  # æ™´ã‚Œ
    "101": "ğŸŒ¤ï¸",  # æ™´ã‚Œæ™‚ã€…ãã‚‚ã‚Š
    "200": "â˜ï¸",  # ãã‚‚ã‚Š
    "201": "â›…",  # ãã‚‚ã‚Šæ™‚ã€…æ™´ã‚Œ
    "202": "ğŸŒ§ï¸",  # ãã‚‚ã‚Šä¸€æ™‚é›¨
    "300": "ğŸŒ§ï¸",  # é›¨
    "301": "ğŸŒ¦ï¸",  # é›¨æ™‚ã€…æ™´ã‚Œ
    "400": "ğŸŒ¨ï¸",  # é›ª
}


def get_weather_data():
    """
    æ°—è±¡åºã®APIã‹ã‚‰æ±äº¬ã®å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹

    Returns
    -------
    dict
        å¤©æ°—ãƒ‡ãƒ¼ã‚¿ï¼ˆæ°—æ¸©ã¨å¤©æ°—ã‚³ãƒ¼ãƒ‰ï¼‰
    """
    try:
        response = requests.get(
            "https://www.jma.go.jp/bosai/forecast/data/forecast/130000.json", timeout=5
        )
        response.raise_for_status()
        data = response.json()

        # æ±äº¬åœ°æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        tokyo = next(
            (
                area
                for area in data[0]["timeSeries"][2]["areas"]
                if area["area"]["name"] == "æ±äº¬"
            ),
            None,
        )
        tokyo_weather = next(
            (
                area
                for area in data[0]["timeSeries"][0]["areas"]
                if area["area"]["code"] == "130010"
            ),
            None,
        )

        if tokyo and tokyo_weather:
            # ç¾åœ¨ã®æ°—æ¸©ï¼ˆtemps[0]ãŒæœ€ä½æ°—æ¸©ã€temps[1]ãŒæœ€é«˜æ°—æ¸©ï¼‰
            temps = tokyo["temps"]
            weather_code = tokyo_weather["weatherCodes"][0]
            weather_icon = WEATHER_ICONS.get(weather_code, "")

            return {
                "temp": temps[0],
                "weather_code": weather_code,
                "weather_icon": weather_icon,
            }
    except Exception as e:
        print(f"Error fetching weather data: {e}")

    # ã‚¨ãƒ©ãƒ¼æ™‚ã‚„ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
    return {
        "temp": "--",
        "weather_code": "100",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ™´ã‚Œ
        "weather_icon": WEATHER_ICONS.get("100", "â˜€ï¸"),
    }


def extract_links(text: str) -> list[str]:
    """Markdownãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹"""
    # Markdownå½¢å¼ã®ãƒªãƒ³ã‚¯ [text](url) ã‚’æŠ½å‡º
    markdown_links = re.findall(r"\[([^\]]+)\]\(([^)]+)\)", text)
    # ã‚‚ã—[text]ã®éƒ¨åˆ†ãŒ[Image]ã¾ãŸã¯[Video]ã®å ´åˆã¯ã€ãã®éƒ¨åˆ†ã‚’é™¤å¤–
    markdown_links = [
        (text, url)
        for text, url in markdown_links
        if not text.startswith("[Image]") and not text.startswith("[Video]")
    ]
    # é€šå¸¸ã®URLã‚‚æŠ½å‡º
    urls = re.findall(r"(?<![\(\[])(https?://[^\s\)]+)", text)

    return [url for _, url in markdown_links] + urls


def fetch_url_content(url: str) -> str | None:
    """URLã®å†…å®¹ã‚’å–å¾—ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã‚’å‰Šé™¤
        for element in soup(["script", "style", "nav", "header", "footer"]):
            element.decompose()

        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆarticle, main, ã¾ãŸã¯æœ¬æ–‡è¦ç´ ï¼‰
        main_content = soup.find("article") or soup.find("main") or soup.find("body")
        if main_content:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
            text = " ".join(main_content.get_text(separator=" ").split())
            # é•·ã™ãã‚‹å ´åˆã¯æœ€åˆã®1000æ–‡å­—ã«åˆ¶é™
            return text[:1000] + "..." if len(text) > 1000 else text

        return None
    except Exception as e:
        print(f"Error fetching URL {url}: {e}")
        return None


def fetch_markdown(app_name: str, date_str: str) -> str:
    """
    æŒ‡å®šã•ã‚ŒãŸã‚¢ãƒ—ãƒªåã¨æ—¥ä»˜ã®S3ä¸Šã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã€
    Markdownã‚’HTMLã«å¤‰æ›ã—ã¦è¿”ã™ã€‚
    """
    key = f"{app_name}/{date_str}.md"
    try:
        response = s3_client.get_object(Bucket=BUCKET_NAME, Key=key)
        md_content = response["Body"].read().decode("utf-8")
        # ãƒ‡ãƒãƒƒã‚°ç”¨ã«Markdownã®å†…å®¹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        print(f"Fetched markdown for {key}:")
        print(md_content[:500])  # æœ€åˆã®500æ–‡å­—ã ã‘è¡¨ç¤º
        return md_content
    except Exception as e:
        return f"Error fetching {key}: {e}"


@app.get("/", response_class=HTMLResponse)
async def index(request: Request, date: str = None):
    if date is None:
        date = datetime.date.today().strftime("%Y-%m-%d")
    contents = {name: fetch_markdown(name, date) for name in app_names}

    # å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    weather_data = get_weather_data()

    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "contents": contents,
            "date": date,
            "app_names": app_names,
            "weather_data": weather_data,
        },
    )


@app.get("/api/weather", response_class=JSONResponse)
async def get_weather():
    """å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return get_weather_data()


_MESSAGE = """
ä»¥ä¸‹ã®è¨˜äº‹ã«é–¢é€£ã—ã¦ã€æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ç”¨ã„ã¦äº‹å®Ÿã‚’ç¢ºèªã—ãªãŒã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦ã§ãã‚‹ã ã‘è©³ç´°ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ãªãŠã€å›ç­”ã¯Markdownå½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

[è¨˜äº‹]

{markdown}

{additional_context}

[ãƒãƒ£ãƒƒãƒˆå±¥æ­´]

'''
{chat_history}
'''

[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æ–°ã—ã„è³ªå•]

'''
{message}
'''

ãã‚Œã§ã¯ã€å›ç­”ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
"""


@app.post("/chat/{topic_id}")
async def chat(topic_id: str, request: Request):
    data = await request.json()
    message = data.get("message")
    markdown = data.get("markdown")
    chat_history = data.get("chat_history", "ãªã—")  # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å—ã‘å–ã‚‹

    # markdownã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    links = extract_links(markdown) + extract_links(message)

    # ãƒªãƒ³ã‚¯ã®å†…å®¹ã‚’å–å¾—
    additional_context = []
    for url in links:
        if content := fetch_url_content(url):
            additional_context.append(f"- Content from {url}:\n\n'''{content}'''\n\n")

    # è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã€markdownã«è¿½åŠ 
    if additional_context:
        additional_context = (
            "\n\n[è¨˜äº‹ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å«ã¾ã‚Œã‚‹ãƒªãƒ³ã‚¯ã®å†…å®¹](ã†ã¾ãå–å¾—ã§ãã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)\n\n"
            + "\n\n".join(additional_context)
        )
    else:
        additional_context = ""

    formatted_message = _MESSAGE.format(
        markdown=markdown,
        additional_context=additional_context,
        chat_history=chat_history,
        message=message,
    )

    gemini_client = create_client(use_search=True)
    response_text = gemini_client.chat_with_search(formatted_message)

    return {"response": response_text}


# AWS Lambdaä¸Šã§FastAPIã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒãƒ³ãƒ‰ãƒ©
lambda_handler = Mangum(app)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8080)
